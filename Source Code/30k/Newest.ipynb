{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddddd79-2db3-42c3-868b-17aa0a2ed86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GRU, Concatenate, Reshape, Dropout, add\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5ac4df-e217-4c8a-84e2-ae56e5a29115",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'Dataset'\n",
    "WORKING_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342c35d1-bead-4170-b8c8-1f73d12f82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e232c3a-02ea-4888-ba34-444fc5a5736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of image to captions\n",
    "mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016bca3b-2254-49d7-a419-ff77437eada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69d076d019a41faaad944ecf980a71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption)\n",
    "\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a10d23c-d87a-4f35-b67b-bc3fb3575e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean function\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i].lower()\n",
    "            caption = re.sub(r'[^a-zA-Z]', ' ', caption)\n",
    "            caption = re.sub(r'\\s+', ' ', caption)\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5be291dd-8eaf-40a5-9dba-9689c0be17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d7a27e-8e55-403c-9c61-bef676dfa934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all captions\n",
    "all_captions = [caption for key in mapping for caption in mapping[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7036d2-4ed2-4f10-8384-8295e8756796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa3d128-830b-4e59-aa7b-020892469c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum length of the caption\n",
    "max_length = max(len(caption.split()) for caption in all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a289a6-e251-4f0d-bdd2-e2820ccb28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image IDs\n",
    "image_ids = list(mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbe2051-598d-4f44-847a-224275729684",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'CustomImages'\n",
    "WORKING_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a899a8a0-c051-40d7-9938-f51872be72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNetV2B0 model\n",
    "base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7b4e52-f704-4c8e-9efd-071f6fdafd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models\n",
    "# Create a sequential model\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28661d8-03b2-42a9-8f6e-11d831715f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from image\n",
    "features = {}\n",
    "directory = BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a506f9db-95f8-40e4-a87d-5aa76d85f678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d4a8802ed344b395a1801aa0332eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # Load the image from file\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # Extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "\n",
    "    # Store feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7330bb-5a3e-4b1b-9579-01b4541b47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f735ba1-4637-4991-8f3c-6130932a02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d9286f-d879-4e09-9fa7-06b64c591213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('best_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83529e24-11cc-4a78-8b97-ff6fe5f22202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert index to word\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86276ff-5f2e-4b43-bcda-b8a1f95a4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    in_text = in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\")\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61bbf49e-5a77-418f-90a4-3978e2689e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    # Load the image\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, image_name)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print('Predicted Caption: ', y_pred, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f3f801c-5aec-40eb-9dc0-bd90aed1f1ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, 74), found shape=(None, 35)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampleimg1.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m generate_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampleimg3.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m generate_caption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampleimg5.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m, in \u001b[0;36mgenerate_caption\u001b[1;34m(image_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Predict the caption\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36mpredict_caption\u001b[1;34m(model, image, tokenizer, max_length)\u001b[0m\n\u001b[0;32m      6\u001b[0m sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([in_text])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m sequence \u001b[38;5;241m=\u001b[39m pad_sequences([sequence], maxlen\u001b[38;5;241m=\u001b[39mmax_length)\n\u001b[1;32m----> 8\u001b[0m yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m yhat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(yhat)\n\u001b[0;32m     10\u001b[0m word \u001b[38;5;241m=\u001b[39m idx_to_word(yhat, tokenizer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file5j7w6r97.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2341, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2327, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2315, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py\", line 2283, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\mayur\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, 74), found shape=(None, 35)\n"
     ]
    }
   ],
   "source": [
    "generate_caption(\"sampleimg1.jpg\")\n",
    "generate_caption(\"sampleimg3.jpeg\")\n",
    "generate_caption(\"sampleimg5.jpg\")\n",
    "generate_caption(\"sampleimg7.jpeg\")\n",
    "generate_caption(\"sampleimg8.jpeg\")\n",
    "generate_caption(\"sampleimg9.jpeg\")\n",
    "generate_caption(\"sampleimg11.jpeg\")\n",
    "generate_caption(\"sampleimg12.jpeg\")\n",
    "generate_caption(\"sampleimg13.jpeg\")\n",
    "generate_caption(\"sampleimg14.jpeg\")\n",
    "generate_caption(\"sampleimg15.jpeg\")\n",
    "generate_caption(\"sampleimg16.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944cf25-5518-4d4c-8ea7-3d9be5cc2ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
