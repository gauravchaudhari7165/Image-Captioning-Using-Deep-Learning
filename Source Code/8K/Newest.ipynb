{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddddd79-2db3-42c3-868b-17aa0a2ed86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2B0, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, GRU, Concatenate, Reshape, Dropout, add\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ac4df-e217-4c8a-84e2-ae56e5a29115",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'Dataset'\n",
    "WORKING_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c35d1-bead-4170-b8c8-1f73d12f82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
    "    next(f)\n",
    "    captions_doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e232c3a-02ea-4888-ba34-444fc5a5736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping of image to captions\n",
    "mapping = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c32b1e-d95e-467c-91ae-be9fbd76280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016bca3b-2254-49d7-a419-ff77437eada6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1a0acd53f54f518a3578799b655188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process lines\n",
    "for line in tqdm(captions_doc.split('\\n')):\n",
    "    tokens = line.split(',')\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    image_id, caption = tokens[0], tokens[1:]\n",
    "    image_id = image_id.split('.')[0]\n",
    "    caption = \" \".join(caption)\n",
    "\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a10d23c-d87a-4f35-b67b-bc3fb3575e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the clean function\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i].lower()\n",
    "            caption = re.sub(r'[^a-zA-Z]', ' ', caption)\n",
    "            caption = re.sub(r'\\s+', ' ', caption)\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be291dd-8eaf-40a5-9dba-9689c0be17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text\n",
    "clean(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94d7a27e-8e55-403c-9c61-bef676dfa934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all captions\n",
    "all_captions = [caption for key in mapping for caption in mapping[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7036d2-4ed2-4f10-8384-8295e8756796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa3d128-830b-4e59-aa7b-020892469c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum length of the caption\n",
    "max_length = max(len(caption.split()) for caption in all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a289a6-e251-4f0d-bdd2-e2820ccb28fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image IDs\n",
    "image_ids = list(mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbe2051-598d-4f44-847a-224275729684",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'CustomImages'\n",
    "WORKING_DIR = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a899a8a0-c051-40d7-9938-f51872be72bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hppav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hppav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
      "20004864/24274472 [=======================>......] - ETA: 1s"
     ]
    }
   ],
   "source": [
    "# Load EfficientNetV2B0 model\n",
    "base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7b4e52-f704-4c8e-9efd-071f6fdafd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models\n",
    "# Create a sequential model\n",
    "model = models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.GlobalAveragePooling2D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28661d8-03b2-42a9-8f6e-11d831715f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from image\n",
    "features = {}\n",
    "directory = BASE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506f9db-95f8-40e4-a87d-5aa76d85f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_name in tqdm(os.listdir(directory)):\n",
    "    # Load the image from file\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    image = load_img(img_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "    # Extract features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "\n",
    "    # Get image ID\n",
    "    image_id = img_name.split('.')[0]\n",
    "\n",
    "    # Store feature\n",
    "    features[image_id] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7330bb-5a3e-4b1b-9579-01b4541b47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store features in pickle\n",
    "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f735ba1-4637-4991-8f3c-6130932a02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from pickle\n",
    "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9286f-d879-4e09-9fa7-06b64c591213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83529e24-11cc-4a78-8b97-ff6fe5f22202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert index to word\n",
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86276ff-5f2e-4b43-bcda-b8a1f95a4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    in_text = 'startseq'\n",
    "\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += \" \" + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbf49e-5a77-418f-90a4-3978e2689e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    # Load the image\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, image_name)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Predict the caption\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print('Predicted Caption: ', y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f801c-5aec-40eb-9dc0-bd90aed1f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"sampleimg1.jpg\")\n",
    "generate_caption(\"sampleimg3.jpeg\")\n",
    "generate_caption(\"sampleimg5.jpg\")\n",
    "generate_caption(\"sampleimg7.jpeg\")\n",
    "generate_caption(\"sampleimg8.jpeg\")\n",
    "generate_caption(\"sampleimg9.jpeg\")\n",
    "generate_caption(\"sampleimg11.jpeg\")\n",
    "generate_caption(\"sampleimg12.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f944cf25-5518-4d4c-8ea7-3d9be5cc2ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
